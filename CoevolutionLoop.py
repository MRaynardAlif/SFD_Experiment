# -*- coding: utf-8 -*-
"""
Created on Fri Oct 24 17:31:37 2025

@author: Raynard
"""

"""
coevolution_loop_v6.py
End-to-end coevolution loop using:
 - SyntheticDataGenerator_v6.generate_ac_dataset
 - RandomForestClassifier.train_rf_model (must exist in your RF file)
 - RFClassifierValidation.evaluate_model (must exist)
 - CoevolutionController.adjust_params to update per-condition params
Saves history, params, best model.
"""
"""
import os
import json
import joblib
import pandas as pd
from SyntheticDataGenerator import SyntheticDataGenerator
from RandomForestClassifier import RFModel
from RFClassifierValidation import RFValidator
from CoevolutionController import CoevolutionController

BASELINE_SYNTHETIC_PATH = r"R:\Mine\AllOfMe\T.A. Penelitian\ML_Model\Model-DataCoevolution\SyntheticStatic\ScaledSyntheticTrainingDataset.csv"
REAL_VALIDATION_PATH = r"R:/Mine/AllOfMe/T.A. Penelitian/ML_Model/RealData/ScaledRealValidationDataset.csv"
OUTPUT_DIR = r"R:/Mine/AllOfMe/T.A. Penelitian/ML_Model/Model-DataCoevolution/CoevolutionResult"

MAX_ITER = 100
EARLY_STOP_PATIENCE = 25

if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

BEST_MODEL_PATH = os.path.join(OUTPUT_DIR, "best_model.pkl")
BEST_SYNTH_PATH = os.path.join(OUTPUT_DIR, "best_synthetic_data.csv")
BEST_PARAMS_PATH = os.path.join(OUTPUT_DIR, "best_controller_params.json")
HISTORY_PATH = os.path.join(OUTPUT_DIR, "coevolution_history.json")

# === INITIALIZATION ===
print("=== Starting Coevolution Loop ===")

generator = SyntheticDataGenerator(BASELINE_SYNTHETIC_PATH)
controller = CoevolutionController(learning_rate=0.2)
validator = RFValidator(REAL_VALIDATION_PATH)
rf_trainer = RFModel()

best_f1 = 0
no_improve_count = 0
history = []

# === MAIN LOOP ===
for iteration in range(1, MAX_ITER + 1):
    print(f"\n--- Iteration {iteration} ---")

    # Generate evolved dataset
    params = controller.params
    synth_path = os.path.join(OUTPUT_DIR, f"synthetic_iter_{iteration}.csv")
    evolved_df = generator.evolve_dataset(params, synth_path)

    # Train model on evolved data
    model, f1_train = rf_trainer.train(synth_path)

    # Validate on real data with feature alignment
    f1_real = validator.validate(model, feature_reference_df=evolved_df)

    # Record metrics
    history.append({
        "iteration": iteration,
        "f1_train": f1_train,
        "f1_real": f1_real,
        "params": dict(controller.params)
    })

    # Check for improvement
    if f1_real > best_f1:
        best_f1 = f1_real
        no_improve_count = 0

        # === Save best model ===
        joblib.dump(model, BEST_MODEL_PATH)

        # === Save best evolved synthetic dataset ===
        evolved_df.to_csv(BEST_SYNTH_PATH, index=False)

        # === Save best controller parameters ===
        with open(BEST_PARAMS_PATH, "w") as f:
            json.dump(controller.params, f, indent=15)

        print(f"[Loop] ðŸ”¥ New best F1={f1_real:.4f}")
        print(f" â”œâ”€ Model saved: {BEST_MODEL_PATH}")
        print(f" â”œâ”€ Synthetic data saved: {BEST_SYNTH_PATH}")
        print(f" â””â”€ Controller params saved: {BEST_PARAMS_PATH}")

        # === Reload baseline for next iteration ===
        print("[Loop] Reloading baseline from best evolved dataset for next iteration...")
        generator.baseline = pd.read_csv(BEST_SYNTH_PATH)

    else:
        no_improve_count += 1
        print(f"[Loop] No improvement ({no_improve_count}/{EARLY_STOP_PATIENCE})")

    # Update controller parameters
    controller.update({"NORMAL": f1_real})

    # Save history checkpoint
    with open(HISTORY_PATH, "w") as f:
        json.dump(history, f, indent=4)

    print(f"[Loop] Iter {iteration} done â€” F1_real={f1_real:.4f} | Best F1={best_f1:.4f}")

    # Early stopping condition
    if no_improve_count >= EARLY_STOP_PATIENCE:
        print(f"[Loop] Early stopping triggered after {iteration} iterations (no improvement).")
        break

print("\n=== Coevolution Completed ===")
print(f"Best real F1 achieved: {best_f1:.4f}")
print(f"Best evolved dataset path: {BEST_SYNTH_PATH}")
print(f"Best model path: {BEST_MODEL_PATH}")
"""
#%%

import os
import json
import joblib
import pandas as pd
import numpy as np

from SyntheticDataGenerator import SyntheticDataGenerator
from RandomForestClassifier import RFModel
from RFClassifierValidation import RFValidator
from CoevolutionController import CoevolutionController

# --- Feature Divergence Function (Simplified) ---
def feature_divergence_score(real_df, synth_df):
    """Calculates a simple divergence score."""
    real_numeric = real_df.select_dtypes(include=np.number)
    synth_numeric = synth_df.select_dtypes(include=np.number)
    common_cols = list(set(real_numeric.columns) & set(synth_numeric.columns))

    if not common_cols:
        return 1.0  # Max divergence if no columns match

    real_means = real_numeric[common_cols].mean()
    synth_means = synth_numeric[common_cols].mean()

    abs_diff = (real_means - synth_means).abs()
    real_scale = real_means.abs().mean() + 1e-6  # Avoid division by zero

    normalized_divergence = (abs_diff / real_scale).mean()
    return normalized_divergence


# --- Pareto Front Manager ---
class ParetoManager:
    """Tracks the set of best non-dominated solutions (the Pareto front)."""
    def __init__(self, save_path):
        self.save_path = save_path
        self.front = []  # List of solution dictionaries

    def dominates(self, a, b):
        """Checks if solution 'a' dominates solution 'b'."""
        better_or_equal = (a["f1"] >= b["f1"] and a["precision"] >= b["precision"] and a["recall"] >= b["recall"]
        )
        strictly_better = (a["f1"] > b["f1"] and (a["precision"] > b["precision"] or a["recall"] > b["recall"])
        )
        return better_or_equal and strictly_better

    def update(self, candidate):
        """
        Adds a new candidate solution to the Pareto front, removing any
        solutions it now dominates.
        """
        new_front = [c for c in self.front if not self.dominates(candidate, c)]
        dominated_by_existing = any(self.dominates(c, candidate) for c in self.front)

        if not dominated_by_existing:
            new_front.append(candidate)

        self.front = new_front

        try:
            with open(self.save_path, "w") as f:
                json.dump(self.front, f, indent=4)
        except Exception as e:
            print(f"[Pareto] Error saving front: {e}")


# --- Composite + Hybrid Score ---
def calculate_hybrid_score(report, divergence):
    """
    Calculates the composite and hybrid scores based on your defined weights.
    """
    f1 = report["weighted avg"]["f1-score"]
    precision = report["weighted avg"]["precision"]
    recall = report["weighted avg"]["recall"]
    composite = 0.2 * f1 + 0.3 * precision + 0.5 * recall
    hybrid = 0.9 * composite + 0.1 * (1 - divergence)
    return hybrid, composite, f1, precision, recall


# --- Configuration ---
BASELINE_SYNTH_PATH = r"R:\Mine\AllOfMe\T.A. Penelitian\ML_Model\Model-DataCoevolution\SyntheticStatic\ScaledSyntheticTrainingDataset.csv"
REAL_VALID_PATH = r"R:/Mine/AllOfMe/T.A. Penelitian/ML_Model/RealData/ScaledRealValidationDataset.csv"
OUTPUT_DIR = r"R:/Mine/AllOfMe/T.A. Penelitian/ML_Model/Model-DataCoevolution/CoevolutionResult"

MAX_ITER = 210
EARLY_STOP_PATIENCE = 70

# --- Create output directory ---
os.makedirs(OUTPUT_DIR, exist_ok=True)
BEST_MODEL_PATH = os.path.join(OUTPUT_DIR, "best_model.pkl")
BEST_SYNTH_PATH = os.path.join(OUTPUT_DIR, "best_synthetic_data.csv")
BEST_PARAMS_PATH = os.path.join(OUTPUT_DIR, "best_controller_params.json")
PARETO_FRONT_PATH = os.path.join(OUTPUT_DIR, "pareto_front_solutions.json")
HISTORY_PATH = os.path.join(OUTPUT_DIR, "coevolution_history.json")

# --- Initialize Components ---
print("=== Starting Modelâ€“Data Coevolution (Simulated Annealing) ===")
generator = SyntheticDataGenerator(BASELINE_SYNTH_PATH)
controller = CoevolutionController(
    initial_step_size=0.1,
    start_temperature=0.2,
    cooling_rate=0.98,
    random_state=42,
)
rf_trainer = RFModel()
validator = RFValidator(REAL_VALID_PATH, output_dir=OUTPUT_DIR)
pareto = ParetoManager(PARETO_FRONT_PATH)

try:
    real_data = pd.read_csv(REAL_VALID_PATH)
    print(f"Loaded real validation data for divergence calculation: {real_data.shape}")
except Exception as e:
    print(f"CRITICAL ERROR: Could not load real validation data from {REAL_VALID_PATH}. {e}")
    exit()


# --- Main Loop ---
history = []

for iteration in range(1, MAX_ITER + 1):
    print(f"\n--- Iteration {iteration}/{MAX_ITER} ---")

    # 1. Controller PROPOSES new parameters
    params = controller.propose_new_params()

    synth_path = os.path.join(OUTPUT_DIR, f"synthetic_iter_{iteration}.csv")

    # 2. Generate, Train, and Validate
    try:
        evolved_df = generator.evolve_dataset(params, synth_path)
        model, f1_train = rf_trainer.train(synth_path)
        report = validator.validate(model, feature_reference_df=evolved_df)
    except Exception as e:
        print(f"[Loop] ERROR during train/validate step: {e}. Skipping iteration.")
        controller.accept_or_reject_proposal(-np.inf)
        continue

    # 3. Calculate Scores
    divergence = feature_divergence_score(real_data, evolved_df)
    hybrid, composite, f1, precision, recall = calculate_hybrid_score(report, divergence)

    print(
        f"[Loop] Score: Hybrid={hybrid:.4f} | Composite={composite:.4f} | Divergence={divergence:.4f}"
    )

    # 4. Controller DECIDES to accept or reject the proposal
    controller.accept_or_reject_proposal(hybrid)

    # 5. Update Pareto Front
    candidate = {
        "f1": f1,
        "precision": precision,
        "recall": recall,
        "iteration": iteration,
        "hybrid_score": hybrid,
        "params": controller.current_params,
    }
    pareto.update(candidate)

    # 6. Check if the ALL-TIME best score was updated this iteration
    if controller.best_score == hybrid:
        print(f"[Loop] ðŸ”¥ New best Hybrid Score found={hybrid:.4f} (F1={f1:.4f})")

        joblib.dump(model, BEST_MODEL_PATH)
        evolved_df.to_csv(BEST_SYNTH_PATH, index=False)
        with open(BEST_PARAMS_PATH, "w") as f: json.dump(controller.best_params, f, indent=4)

        validator.validate(
            model,
            feature_reference_df=evolved_df,
            save_best=True,
            normalize=False  
        )

        generator.baseline = evolved_df

    # 7. Log History
    history.append(
        {
            "iteration": iteration,
            "f1_train": f1_train,
            "f1_real": f1,
            "precision": precision,
            "recall": recall,
            "divergence": divergence,
            "hybrid_score_tested": hybrid,
            "current_accepted_score": controller.current_score,
            "best_all_time_score": controller.best_score,
            "temperature": controller.temperature,
            "params_tested": params,
        }
    )
    try:
        with open(HISTORY_PATH, "w") as f:
            json.dump(history, f, indent=4)
    except Exception as e:
        print(f"[History] Error saving history: {e}")

    # 8. Check for Early Stopping
    if iteration > EARLY_STOP_PATIENCE:
        recent_best_scores = [h["best_all_time_score"] for h in history[-EARLY_STOP_PATIENCE:]]
        if len(set(recent_best_scores)) == 1:
            print(
                f"[Loop] Early stopping triggered: Best score has not changed for {EARLY_STOP_PATIENCE} iterations."
            )
            break

print("\n=== Coevolution Complete (Simulated Annealing) ===")
print(f"Best Hybrid Score Found: {controller.best_score:.4f}")
print(f"Best Model Path: {BEST_MODEL_PATH}")
print(f"Best Synthetic Data: {BEST_SYNTH_PATH}")
print(f"Best Parameters saved to: {BEST_PARAMS_PATH}")
print(f"Pareto Front saved to: {PARETO_FRONT_PATH}")